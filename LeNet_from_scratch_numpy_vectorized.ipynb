{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.genfromtxt('MNIST_CSV/mnist_train.csv', delimiter=',',dtype='int')\n",
    "test = np.genfromtxt('MNIST_CSV/mnist_test.csv', delimiter=',',dtype='int')\n",
    "\n",
    "X_train = train[:,1:].reshape(-1,1,28,28)\n",
    "y_train = train[:,0]\n",
    "\n",
    "X_test = test[:,1:].reshape(-1,1,28,28)\n",
    "y_test = test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_buf = np.pad(X_train,((0,0),(0,0),(2,2),(2,2)))\n",
    "X_test_buf = np.pad(X_test,((0,0),(0,0),(2,2),(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_buf\n",
    "X_test = X_test_buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class LayerInput(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = 'linear'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}'\n",
    "\n",
    "class LayerConv2D(Layer):\n",
    "\n",
    "    def __init__(self,kernel_size,n_kernels,stride=1,padding=0,W=None,b=None,activation='linear',use_bias=True):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.n_kernels = n_kernels\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self._W_and_b = False\n",
    "        if type(W) == np.ndarray:\n",
    "            self.W = W\n",
    "            self.n_channels = W.shape[1]\n",
    "            self._W_and_b = True\n",
    "        if type(b) == np.ndarray:\n",
    "            self.b = b\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}: kernel_size = {self.kernel_size}, stride = {self.stride}, padding = {self.padding}, n_kernels = {self.n_kernels}, activation = {self.activation}'\n",
    "\n",
    "    def initialize_weights_and_biases(self):\n",
    "        self.W = np.random.uniform(-0.5,0.5,(self.n_kernels,self.n_channels,self.kernel_size,self.kernel_size))\n",
    "        self.b = np.random.uniform(-0.5,0.5,(1,self.n_kernels,1,1))\n",
    "\n",
    "    def update_weights_and_biases(self,dW,db,lr=0.001):\n",
    "        self.W -= lr*dW\n",
    "        self.b -= lr*db\n",
    "\n",
    "class LayerMaxPool2D(Layer):\n",
    "\n",
    "    def __init__(self,kernel_size,stride=2,padding=0):\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.activation = 'linear'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}: kernel_size = {self.kernel_size}, stride = {self.stride}, padding = {self.padding}'\n",
    "    \n",
    "class LayerAveragePool2D(Layer):\n",
    "\n",
    "    def __init__(self,kernel_size,stride=2,padding=0):\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.activation = 'linear'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}: kernel_size = {self.kernel_size}, stride = {self.stride}, padding = {self.padding}'\n",
    "        \n",
    "class LayerDense(Layer):\n",
    "\n",
    "    def __init__(self,n_neurons,activation='linear'):\n",
    "        self.n_neurons = n_neurons\n",
    "        self.activation = activation\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}: activation = {self.activation}'\n",
    "    \n",
    "    def initialize_weights_and_biases(self,n_neurons_prev):\n",
    "        self.W = np.random.rand(self.n_neurons, n_neurons_prev) - 0.5\n",
    "        self.b = np.random.rand(self.n_neurons, 1) - 0.5\n",
    "\n",
    "    def update_weights_and_biases(self,dW,db,lr=0.01):\n",
    "        self.W -= lr*dW\n",
    "        self.b -= lr*db\n",
    "        \n",
    "class LayerFlatten(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = 'linear'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet:\n",
    "\n",
    "    def __init__(self,file=None,layers=None):\n",
    "        if file:\n",
    "            self.load(file)\n",
    "        elif layers:\n",
    "            self.layers = layers\n",
    "        else:\n",
    "            self.layers = [LayerInput()]\n",
    "            self.score = 0.0\n",
    "            self.best_score = 0.0\n",
    "            self.best = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}'\n",
    "\n",
    "    def add_layer(self,layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def compile(self,X_shape):\n",
    "        self.activations = []\n",
    "        if len(X_shape) == 4:\n",
    "            _, channels, height, width = X_shape\n",
    "            input_flat = False\n",
    "        elif len(X_shape) == 2:\n",
    "            n_neurons, _ = X_shape\n",
    "            input_flat = True\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            self.activations.append(layer.activation)\n",
    "            if type(layer) == LayerInput:\n",
    "                layer.input_shape = (n_neurons, None) if input_flat else (None, channels, height, width)\n",
    "                layer.output_shape = (n_neurons, None) if input_flat else (None, channels, height, width)\n",
    "                layer.n_trainable_parameters = 0\n",
    "            if type(layer) == LayerConv2D:\n",
    "                if layer._W_and_b:\n",
    "                    if layer.n_channels != channels:\n",
    "                        raise ValueError(f'Layer {i} is incopatible with input')\n",
    "                else:\n",
    "                    layer.n_channels = channels\n",
    "                    layer.initialize_weights_and_biases()\n",
    "                    layer._W_and_b = True\n",
    "                new_height = int((height - layer.kernel_size + 2 * layer.padding)/layer.stride) + 1\n",
    "                new_width = int((width - layer.kernel_size + 2 * layer.padding)/layer.stride) + 1\n",
    "                layer.input_shape = (None, channels, height, width)\n",
    "                layer.output_shape = (None, layer.n_kernels, new_height, new_width)\n",
    "                layer.n_trainable_parameters = (layer.kernel_size**2 * layer.n_channels + 1) * layer.n_kernels\n",
    "                _, channels, height, width = layer.output_shape\n",
    "            if type(layer) == LayerMaxPool2D:\n",
    "                new_height = int((height - layer.kernel_size + 2 * layer.padding)/layer.stride) + 1\n",
    "                new_width = int((width - layer.kernel_size + 2 * layer.padding)/layer.stride) + 1\n",
    "                layer.input_shape = (None, channels, height, width)\n",
    "                layer.output_shape = (None, channels, new_height, new_width)\n",
    "                layer.n_trainable_parameters = 0\n",
    "                _, channels, height, width = layer.output_shape\n",
    "            if type(layer) == LayerAveragePool2D:\n",
    "                new_height = int((height - layer.kernel_size + 2 * layer.padding)/layer.stride) + 1\n",
    "                new_width = int((width - layer.kernel_size + 2 * layer.padding)/layer.stride) + 1\n",
    "                layer.input_shape = (None, channels, height, width)\n",
    "                layer.output_shape = (None, channels, new_height, new_width)\n",
    "                layer.n_trainable_parameters = 0\n",
    "                _, channels, height, width = layer.output_shape\n",
    "            if type(layer) == LayerFlatten:\n",
    "                n_neurons = channels*height*width\n",
    "                layer.input_shape = (None, channels, height, width)\n",
    "                layer.output_shape = (n_neurons, None)\n",
    "                layer.n_trainable_parameters = 0\n",
    "                n_neurons, _ = layer.output_shape\n",
    "            if type(layer) == LayerDense:\n",
    "                layer.initialize_weights_and_biases(n_neurons)\n",
    "                layer.input_shape = (n_neurons, None)\n",
    "                layer.output_shape = (layer.n_neurons, None)\n",
    "                layer.n_trainable_parameters = layer.n_neurons * (n_neurons + 1)\n",
    "                n_neurons, _ = layer.output_shape\n",
    "        self.describe()\n",
    "                \n",
    "    def describe(self):\n",
    "        print(f' --------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "        print(f'| Layer |     Layer type      |      Input shape      | Kernel size | Stride | Padding |      Output shape     | Activation | Trainable Parameters |')\n",
    "        print(f' --------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            layer_2D = False\n",
    "            if type(layer) == LayerConv2D:\n",
    "                layer_type = \"{:^21}\".format('LayerConv2D')\n",
    "                layer_2D = True\n",
    "            elif type(layer) == LayerMaxPool2D:\n",
    "                layer_type = \"{:^21}\".format('LayerMaxPool2D')\n",
    "                layer_2D = True\n",
    "            elif type(layer) == LayerAveragePool2D:\n",
    "                layer_type = \"{:^21}\".format('LayerAveragePool2D')\n",
    "                layer_2D = True\n",
    "            elif type(layer) == LayerDense:\n",
    "                layer_type = \"{:^21}\".format('LayerDense')\n",
    "            elif type(layer) == LayerFlatten:\n",
    "                layer_type = \"{:^21}\".format('LayerFlatten')\n",
    "            elif type(layer) == LayerInput:\n",
    "                layer_type = \"{:^21}\".format('LayerInput')\n",
    "            input_shape = \"{:^23}\".format(str(layer.input_shape))\n",
    "            output_shape = \"{:^23}\".format(str(layer.output_shape))\n",
    "            padding = \"{:^9}\".format(layer.padding) if layer_2D else \"{:^9}\".format('None')\n",
    "            stride = \"{:^8}\".format(layer.stride) if layer_2D else \"{:^8}\".format('None')\n",
    "            kshape = \"{:^13}\".format(f'({layer.kernel_size},{layer.kernel_size})') if layer_2D else \"{:^13}\".format('None')\n",
    "            n_trainable_parameters = \"{:^22}\".format(layer.n_trainable_parameters)\n",
    "            activation = \"{:^12}\".format(layer.activation)\n",
    "            print(f'|{\"{:^7}\".format(i)}|{layer_type}|{input_shape}|{kshape}|{stride}|{padding}|{output_shape}|{activation}|{n_trainable_parameters}|')\n",
    "        print(f' --------------------------------------------------------------------------------------------------------------------------------------------------') \n",
    "\n",
    "    def zero_pad(self,matrix, padding):\n",
    "        return np.pad(matrix,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "\n",
    "    def one_hot(self,Y):\n",
    "        Y_gt = np.zeros((Y.size, self.layers[-1].n_neurons))\n",
    "        Y_gt[np.arange(Y.size), Y] = 1\n",
    "        return Y_gt.T\n",
    "\n",
    "    def ReLu(self,A):\n",
    "        return np.maximum(A,0)\n",
    "    \n",
    "    def dReLu(self,Z):\n",
    "        return Z > 0\n",
    "    \n",
    "    def SoftMax(self,A):\n",
    "        return np.exp(A) / sum(np.exp(A))\n",
    "    \n",
    "    def activate(self,A,activation):\n",
    "        if activation == 'relu':\n",
    "            return self.ReLu(A)\n",
    "        if activation == 'softmax':\n",
    "            return self.SoftMax(A)\n",
    "        if activation == 'linear':\n",
    "            return A\n",
    "        \n",
    "    def dActivation(self,Z,activation):\n",
    "        if activation == 'relu':\n",
    "            return self.dReLu(Z)\n",
    "        if activation == 'linear':\n",
    "            return 1\n",
    "    \n",
    "    def forward_convolve(self,A,layer,is_training):\n",
    "        A = self.zero_pad(A,layer.padding)\n",
    "        window = np.lib.stride_tricks.sliding_window_view(A,layer.W.shape[2:4],axis=(2,3))[:,:,::layer.stride,::layer.stride]\n",
    "        Z = np.einsum('bchwkt,fckt->bfhw', window, layer.W)\n",
    "        if layer.use_bias:\n",
    "            Z += layer.b\n",
    "        if is_training:\n",
    "            layer.cache = A, Z\n",
    "        A = self.activate(Z, layer.activation)\n",
    "        return A\n",
    "    \n",
    "    def backward_convolve(self,dA_prev,layer,batch_size,lr,activation):\n",
    "        A_prev, Z_prev =  layer.cache\n",
    "        dZ_prev = dA_prev * self.dActivation(Z_prev,activation)\n",
    "        W_rot = np.rot90(layer.W,k=2,axes=[2,3])\n",
    "        dZ_shape = dZ_prev.shape\n",
    "        dZ_dilated = np.zeros((dZ_shape[0],dZ_shape[1],(dZ_shape[2])+(dZ_shape[2]-1)*(layer.stride-1),(dZ_shape[3])+(dZ_shape[3]-1)*(layer.stride-1)),dtype=dZ_prev.dtype)\n",
    "        dZ_dilated[:,:,::layer.stride,::layer.stride] = dZ_prev\n",
    "        dZ_paded = self.zero_pad(dZ_dilated,layer.W.shape[2]-1)\n",
    "        window=np.lib.stride_tricks.sliding_window_view(dZ_paded,W_rot.shape[2:4],axis=(2,3))\n",
    "        dA = np.einsum('bfhwkt,fckt->bchw', window, W_rot)\n",
    "        shape_diff = A_prev.shape[2] - dA.shape[2]\n",
    "        if shape_diff:\n",
    "            dA = np.pad(dA,((0,0),(0,0),(0,shape_diff),(0,shape_diff)))\n",
    "            A_prev = A_prev[:,:,:-shape_diff,:-shape_diff]\n",
    "        if layer.padding:\n",
    "            dA = dA[:,:,layer.padding:-layer.padding,layer.padding:-layer.padding]\n",
    "        window=np.lib.stride_tricks.sliding_window_view(A_prev,dZ_dilated.shape[2:4],axis=(2,3))\n",
    "        dW = 1/batch_size * np.einsum('bchwkt,bfkt->fchw', window, dZ_dilated)\n",
    "        db = 1/batch_size * np.sum(dZ_prev,axis=(0,2,3),keepdims=True)\n",
    "        layer.update_weights_and_biases(dW,db,lr)\n",
    "        layer.cache = None\n",
    "        return dA\n",
    "    \n",
    "    def forward_maxpool(self,A,layer,is_training):\n",
    "        windows = np.lib.stride_tricks.sliding_window_view(A,(layer.kernel_size,layer.kernel_size),axis=(2,3))[:,:,::layer.stride,::layer.stride]\n",
    "        A_next = np.max(windows,axis=(4,5))\n",
    "        A_next_full_shape = np.max(windows,axis=(4,5),keepdims=True)\n",
    "        mask = (A_next_full_shape == windows).astype(int)\n",
    "        if is_training:\n",
    "            layer.cache = (A, mask, A_next_full_shape.shape)\n",
    "        return A_next\n",
    "\n",
    "    def backward_maxpool(self,dA,layer):\n",
    "        A_prev, mask, dA_full_shape = layer.cache\n",
    "        dA_next = np.ones_like(A_prev)\n",
    "        dA_window = dA.reshape(dA_full_shape) * mask\n",
    "        windows = np.lib.stride_tricks.sliding_window_view(dA_next,(2,2),axis=(2,3),writeable=True)[:,:,::layer.stride,::layer.stride]\n",
    "        windows *= dA_window\n",
    "        layer.cache = None\n",
    "        return dA_next\n",
    "\n",
    "    # def forward_averagepool(self,A,layer,is_training):\n",
    "    #     windows = np.lib.stride_tricks.sliding_window_view(A,(layer.kernel_size,layer.kernel_size),axis=(2,3))[:,:,::layer.stride,::layer.stride]\n",
    "    #     A_next = np.mean(windows,axis=(4,5))\n",
    "    #     A_next_full_shape = np.mean(windows,axis=(4,5),keepdims=True)\n",
    "    #     if is_training:\n",
    "    #         layer.cache = (A, A_next_full_shape.shape)\n",
    "    #     return A_next\n",
    "    \n",
    "    # def backward_averagepool(self,dA,layer):\n",
    "    #     A_prev, dA_full_shape = layer.cache\n",
    "    #     dA_next = np.ones_like(A_prev)\n",
    "    #     dA_window = dA.reshape(dA_full_shape) / layer.kernel_size**2\n",
    "    #     windows = np.lib.stride_tricks.sliding_window_view(dA_next,(2,2),axis=(2,3),writeable=True)[:,:,::layer.stride,::layer.stride]\n",
    "    #     windows *= dA_window\n",
    "    #     layer.cache = None\n",
    "    #     return dA_next\n",
    "    \n",
    "    def forward_averagepool(self,A,layer,is_training):\n",
    "        windows = np.lib.stride_tricks.sliding_window_view(A,(layer.kernel_size,layer.kernel_size),axis=(2,3))[:,:,::layer.stride,::layer.stride]\n",
    "        A_next = np.mean(windows,axis=(4,5))\n",
    "        if is_training:\n",
    "            layer.cache = A\n",
    "        return A_next\n",
    "    \n",
    "    def backward_averagepool(self,dA_prev,layer):\n",
    "        A_prev = layer.cache\n",
    "        dA = dA_prev / layer.kernel_size**2\n",
    "        shape_diff = A_prev.shape[2] - dA.shape[2]\n",
    "        if shape_diff:\n",
    "            dA = np.pad(dA,((0,0),(0,0),(0,shape_diff),(0,shape_diff)))\n",
    "        if layer.padding:\n",
    "            dA = dA[:,:,layer.padding:-layer.padding,layer.padding:-layer.padding]\n",
    "        layer.cache = None\n",
    "        return dA\n",
    "\n",
    "    def forward_flatten(self,A):\n",
    "        batch_size, _, _, _ = A.shape\n",
    "        return A.reshape(batch_size,-1).T\n",
    "    \n",
    "    def backward_flatten(self,dA_prev,layer):\n",
    "        _, channels, height, width = layer.input_shape\n",
    "        dA = dA_prev.T.reshape(-1,channels, height, width)\n",
    "        return dA\n",
    "    \n",
    "    def forward_dense(self,A,Z,layer,is_training=False):\n",
    "        if is_training:\n",
    "            layer.cache = (A,Z)\n",
    "        Z = layer.W.dot(A) + layer.b\n",
    "        A = self.activate(Z,layer.activation)\n",
    "        return A, Z\n",
    "    \n",
    "    def backward_dense(self,dA_prev,layer,batch_size,lr,activation):\n",
    "        A_prev, Z_prev = layer.cache\n",
    "        dW = 1/batch_size * dA_prev.dot(A_prev.T)\n",
    "        db = 1/batch_size * np.sum(dA_prev)\n",
    "        dA = layer.W.T.dot(dA_prev) * self.dActivation(Z_prev,activation)\n",
    "        layer.update_weights_and_biases(dW,db,lr)\n",
    "        layer.cache = None\n",
    "        return dA\n",
    "\n",
    "    def forward(self,A,is_training=False):\n",
    "        Z = A.copy()\n",
    "        for layer in self.layers:\n",
    "            if type(layer) == LayerConv2D:\n",
    "                A = self.forward_convolve(A,layer,is_training)\n",
    "            elif type(layer) == LayerMaxPool2D:\n",
    "                A = self.forward_maxpool(A,layer,is_training)\n",
    "            elif type(layer) == LayerAveragePool2D:\n",
    "                A = self.forward_averagepool(A,layer,is_training)\n",
    "            elif type(layer) == LayerFlatten:\n",
    "                A = self.forward_flatten(A)\n",
    "            elif type(layer) == LayerDense:\n",
    "                A, Z = self.forward_dense(A,Z,layer,is_training)\n",
    "        return A\n",
    "    \n",
    "    def backward(self,dA,batch_size,lr):\n",
    "        for layer, activation in zip(self.layers[1:][::-1],self.activations[:-1][::-1]):\n",
    "            if type(layer) == LayerDense:\n",
    "                dA = self.backward_dense(dA,layer,batch_size,lr,activation)\n",
    "            if type(layer) == LayerFlatten:\n",
    "                dA = self.backward_flatten(dA,layer)\n",
    "            if type(layer) == LayerMaxPool2D:\n",
    "                dA = self.backward_maxpool(dA,layer)\n",
    "            if type(layer) == LayerAveragePool2D:\n",
    "                dA = self.backward_averagepool(dA,layer)\n",
    "            if type(layer) == LayerConv2D:\n",
    "                dA = self.backward_convolve(dA,layer,batch_size,lr,activation)\n",
    "        \n",
    "    def fit(self,X_train,y_train,X_test=None,y_test=None,batch_size=100,epochs=10,lr=0.01,verbose=True):\n",
    "        self.print_progress(verbose,'start')\n",
    "        for epoch in range(epochs):\n",
    "            num_batches = X_train.shape[0] // batch_size\n",
    "            for i in range(0,X_train.shape[0],batch_size):\n",
    "                X_i = X_train[i:i+batch_size]\n",
    "                y_i = y_train[i:i+batch_size]\n",
    "                y_pred_oh = self.forward(X_i,is_training=True)\n",
    "                y_true_oh = self.one_hot(y_i)\n",
    "                dA = y_pred_oh - y_true_oh\n",
    "                self.backward(dA,batch_size,lr)\n",
    "                self.print_progress(verbose,'batch',epoch=epoch,i=i,num_batches=num_batches,batch_size=batch_size,y_i=y_i,y_pred_oh=y_pred_oh)\n",
    "            train_acc = self.val(X_train,y_train)\n",
    "            if (X_test is not None) and (y_test is not None):\n",
    "                test_acc = self.val(X_test,y_test)\n",
    "                self.print_progress(verbose,'epoch',epoch=epoch,train_acc=train_acc,test_acc=test_acc)\n",
    "                self.best_score = max(self.best_score,test_acc)\n",
    "                self.score = test_acc\n",
    "            else:\n",
    "                self.print_progress(verbose,'epoch',epoch=epoch,train_acc=train_acc,test_acc=None)\n",
    "                self.best_score = max(self.best_score,train_acc)\n",
    "                self.score = train_acc\n",
    "        self.print_progress(verbose,'end')\n",
    "\n",
    "    def print_progress(self,verbose,which,epoch=None,i=None,num_batches=None,batch_size=None,y_i=None,y_pred_oh=None,train_acc=None,test_acc=None):\n",
    "        if verbose:\n",
    "            if which == 'start':\n",
    "                print(f'{\"{:^7}\".format(\"epoch\")} | {\"{:^12}\".format(\"progress\")} | {\"{:^10}\".format(\"accuracy\")}')\n",
    "                print(f'{\"{:<53}\".format(\"-\"*53)}')\n",
    "            elif which == 'batch':\n",
    "                print(f'{\"{:^7}\".format(str(epoch))} | [{\"{:<10}\".format(\"#\"*int(i*10/num_batches/batch_size))}] | {\"{:^7}\".format(str(round(self.evaluate_accuracy(y_i,np.argmax(y_pred_oh,axis=0)),4)))}',end='\\r', flush=True)\n",
    "            elif which == 'epoch':\n",
    "                if test_acc is not None:\n",
    "                    print(f'{\"{:^7}\".format(str(epoch))} | [{\"{:<10}\".format(\"#\"*10)}] | train: {\"{:^7}\".format(str(round(train_acc,4)))} test: {\"{:^7}\".format(str(round(test_acc,4)))}')\n",
    "                else:\n",
    "                    print(f'{\"{:^7}\".format(str(epoch))} | [{\"{:<10}\".format(\"#\"*10)}] | train: {\"{:^7}\".format(str(round(train_acc,4)))}')\n",
    "            elif which == 'end':\n",
    "                print(f'{\"{:<53}\".format(\"-\"*53)}')\n",
    "\n",
    "    def predict(self,X):\n",
    "        y = self.forward(X)\n",
    "        return np.argmax(y,0)\n",
    "\n",
    "    def evaluate_accuracy(self,y_true,y_pred):\n",
    "        return (y_true == y_pred).sum() / len(y_true)\n",
    "\n",
    "    def val(self,X_test,y_true):\n",
    "        y_pred = self.predict(X_test)\n",
    "        acc = self.evaluate_accuracy(y_true,y_pred)\n",
    "        return acc\n",
    "                \n",
    "    def save(self,file):\n",
    "        if not file.endswith('.npz'):\n",
    "            file = file +'.npz'\n",
    "        np.savez(file=file,\n",
    "                 layers=self.layers,\n",
    "                 activations=self.activations,\n",
    "                 score = self.score,\n",
    "                 best_score=self.best_score)\n",
    "        print(f'Saved as {file}')\n",
    "        \n",
    "    def load(self,file):\n",
    "        if not file.endswith('.npz'):\n",
    "            file = file + '.npz'\n",
    "            \n",
    "        model_archieve = np.load(file,allow_pickle=True)\n",
    "        self.layers = model_archieve['layers'].tolist()\n",
    "        self.activations = model_archieve['activations'].tolist()\n",
    "        self.score = float(model_archieve['score'])\n",
    "        self.best_score = float(model_archieve['best_score'])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| Layer |     Layer type      |      Input shape      | Kernel size | Stride | Padding |      Output shape     | Activation | Trainable Parameters |\n",
      " --------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|   0   |     LayerInput      |   (None, 1, 32, 32)   |    None     |  None  |  None   |   (None, 1, 32, 32)   |   linear   |          0           |\n",
      "|   1   |     LayerConv2D     |   (None, 1, 32, 32)   |    (5,5)    |   1    |    1    |   (None, 6, 30, 30)   |    relu    |         156          |\n",
      "|   2   | LayerAveragePool2D  |   (None, 6, 30, 30)   |    (2,2)    |   2    |    0    |   (None, 6, 15, 15)   |   linear   |          0           |\n",
      "|   3   |     LayerConv2D     |   (None, 6, 15, 15)   |    (5,5)    |   1    |    0    |  (None, 16, 11, 11)   |    relu    |         2416         |\n",
      "|   4   | LayerAveragePool2D  |  (None, 16, 11, 11)   |    (2,2)    |   2    |    0    |   (None, 16, 5, 5)    |   linear   |          0           |\n",
      "|   5   |     LayerConv2D     |   (None, 16, 5, 5)    |    (5,5)    |   1    |    0    |   (None, 120, 1, 1)   |    relu    |        48120         |\n",
      "|   6   |    LayerFlatten     |   (None, 120, 1, 1)   |    None     |  None  |  None   |      (120, None)      |   linear   |          0           |\n",
      "|   7   |     LayerDense      |      (120, None)      |    None     |  None  |  None   |      (84, None)       |    relu    |        10164         |\n",
      "|   8   |     LayerDense      |      (84, None)       |    None     |  None  |  None   |      (10, None)       |  softmax   |         850          |\n",
      " --------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "\n",
    "model.add_layer(LayerConv2D(kernel_size=5,stride=1,padding=1,n_kernels=6,activation='relu'))\n",
    "model.add_layer(LayerAveragePool2D(kernel_size=2,stride=2,padding=0))\n",
    "model.add_layer(LayerConv2D(kernel_size=5,stride=1,padding=0,n_kernels=16,activation='relu'))\n",
    "model.add_layer(LayerAveragePool2D(kernel_size=2,stride=2,padding=0))\n",
    "model.add_layer(LayerConv2D(kernel_size=5,stride=1,padding=0,n_kernels=120,activation='relu'))\n",
    "model.add_layer(LayerFlatten())\n",
    "model.add_layer(LayerDense(84,activation='relu'))\n",
    "model.add_layer(LayerDense(10,activation='softmax'))\n",
    "\n",
    "model.compile(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch  |   progress   |  accuracy \n",
      "-----------------------------------------------------\n",
      "   0    | [##########] | train: 0.7037  test: 0.7091 \n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train/256,y_train,X_test/256,y_test,batch_size=1000,epochs=1,lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch  |   progress   |  accuracy \n",
      "-----------------------------------------------------\n",
      "   0    | [##########] | train: 0.7856  test: 0.7923 \n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train/256,y_train,X_test/256,y_test,batch_size=1000,epochs=1,lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as LeNet_from_scratch.npz\n"
     ]
    }
   ],
   "source": [
    "model.save('LeNet_from_scratch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet('LeNet_from_scratch.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7091"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.val(X_test/256,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
